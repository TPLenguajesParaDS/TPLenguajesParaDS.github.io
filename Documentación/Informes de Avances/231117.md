# 75.99 Trabajo Profesional

## Avance del proyecto al 17/11/2023

## Índice

- [Informe de Avance de Proyecto](#informe-de-avance-de-proyecto)
  - [Resumen](#resumen)
  - [Hitos y Tareas Completadas](#hitos-y-tareas-completadas)
    - [Hitos Completados](#hitos-completados)
    - [Tareas Completadas](#tareas-completadas)
  - [Problemas, desafíos y/o cambios](#problemas-desafíos-yo-cambios)
  - [Progreso del Cronograma](#progreso-del-cronograma)
  - [Próximos Pasos](#próximos-pasos)
  - [Anexos](#anexos)

## Informe de Avance de Proyecto

### Resumen

Se completaron los scripts pendientes del sprint pasado (SparkR texto + numerico, R numerico y Julia numerico).
Se avanzaron las tareas correspondientes al sprint actual. 
Se armó el entorno de trabajo containerizado para la captura y recolección de métricas, incluida herramienta para generar estadisticas agregadas por operación.

### Hitos y Tareas Completadas

#### Hitos Completados

- Scripts terminados: Julia numerico, R numerico, SparkR numerico, R texto, SparkR texto
- Scripts en progreso/avanzados: ScalaSpark texto, ScalaSpark numerico
- Scrapper de metricas de una corrida
- Primeras corridas/pruebas End-to-end de los datasets small

#### Tareas Completadas

- Armado de entorno dockerizado para recolección de metricas
- Scripts pendientes del sprint pasado terminados: Julia-numerico, R-numerico, SparkR-numerico, R-texto, SparkR-texto

### Problemas, desafíos y/o cambios

- Al momento de realizar las primeras pruebas, encontramos que es necesario realizar ajustes sobre algunas de las operaciones implementadas.
- Adicionalmente nos encontramos con algunos problemas de configuración del ambiente de Spark al correr nuestros scripts de py spark, ya que la carga de trabajo no esta siendo balanceada correctamente entre los nodos master-workers, sobreexigiendo al nodo master por sobre los workers. 
- Detectamos que la operación de 'group by' en el caso numerico para PySpark con el dataset Small (1/4 de la RAM disponible) consume todos los recursos disponibles, finalizando abruptamente la ejecución. 

Analizaremos si debemos redefinir los specs de los containers o redifinir los tamaños de los datasets. A su vez, investigaremos si es un tema de configuración el hecho de la mala distribución de carga en nuestro cluster de Spark.

### Progreso del Cronograma

El proyecto se encuentra con un leve desvío con respecto al cronograma estimado, pero más que nada debido al desvío del sprint anterior y a las tareas no planficadas necesarias para automatizar el benchmark (corridas y analisis de métricas), sumado a incovenientes con las configuraciones.

| Épica             | Tarea                                                                 | Status          |
| :---------------- | :---------------------------------------------------------------------| :-------------- |
| Análisis de datos | Framework: SparkR; dataset: texto                                     | Terminado       |
|                   | Framework: SparkR; dataset: numérico                                  | Terminado       |
|                   | Framework: R; dataset: numérico                                       | Terminado       |
|                   | Framework: Julia; dataset: numérico                                   | Terminado       |
|                   | Framework: ScalaSpark; dataset: texto                                 | En progreso 80% |
|                   | Framework: ScalaSpark; dataset: numérico                              | En progreso 50% |
| No planificados   | Pruebas de concepto para toma de métricas                             | Terminado       |
|                   | Script generador de csv para reporte de métricas (dump de prometheus) | En progreso 80% |
|                   | Script interactivo para correr los scripts                            | En progreso 50% |


### Próximos Pasos

- Para el próximo sprint vamos a:
  - Concluir las tareas de Scala Spark y ajustar los scripts de métricas de reporte y de automatización de corridas.
  - Agregar R a nuestro compose de Spark o hacer un nuevo docker compose para Spark R.
  - Decidir que métricas específicas del reporte utilizar en la comparación y agregar cálculos de rate/increase para métricas del tipo contadores/timers.
  - Generar primeras corridas comparativas con sus reportes y conclusiones
  - Estabilización, bugfixing y estandarización de scripts 

### Anexos

